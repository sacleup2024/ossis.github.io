{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOs/4q81NV3c+coa2F4R9/0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xiaowangzikandashijie/ossis.github.io/blob/main/Llamaindex_LLMs_and_Prompts.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LlamaIndex Bottoms-Up Development - LLMs and Prompts"
      ],
      "metadata": {
        "id": "5YL69VdWN7jN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import os\n",
        "\n",
        "openai.api_key = \"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\"\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"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\""
      ],
      "metadata": {
        "id": "ZZtHI1g8SO5Q"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "current_dir = os.getcwd()\n",
        "file_path = os.path.join(current_dir, \"data\", \"PG.txt\")"
      ],
      "metadata": {
        "id": "R7khwdSnT0be"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(file_path, \"r\") as f:\n",
        "    text = f.read()"
      ],
      "metadata": {
        "id": "-nRQE1rDTKhV"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.llms import OpenAI\n",
        "llm = OpenAI(model=\"gpt-3.5-turbo\", temperature=0)"
      ],
      "metadata": {
        "id": "yTP_6AsgUHj9"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index import Prompt\n",
        "\n",
        "text_qa_template = Prompt(\n",
        "    \"Context information is below.\\n\"\n",
        "    \"---------------------\\n\"\n",
        "    \"{context_str}\\n\"\n",
        "    \"---------------------\\n\"\n",
        "    \"Given the context information and not prior knowledge, \"\n",
        "    \"answer the question: {query_str}\\n\"\n",
        ")\n",
        "\n",
        "refine_template = Prompt(\n",
        "    \"We have the opportunity to refine the original answer \"\n",
        "    \"(only if needed) with some more context below.\\n\"\n",
        "    \"------------\\n\"\n",
        "    \"{context_msg}\\n\"\n",
        "    \"------------\\n\"\n",
        "    \"Given the new context, refine the original answer to better \"\n",
        "    \"answer the question: {query_str}. \"\n",
        "    \"If the context isn't useful, output the original answer again.\\n\"\n",
        "    \"Original Answer: {existing_answer}\"\n",
        ")"
      ],
      "metadata": {
        "id": "dzoYJzFAUjy8"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"How can paul choose the major in university?\"\n",
        "prompt = text_qa_template.format(context_str=text, query_str=question)\n",
        "response = llm.complete(prompt)\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rvHefxFzVflW",
        "outputId": "44d6dfa6-3477-4a94-d0bb-2961079554a0"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "According to the context information, Paul was enrolled in a program at Cornell University that did not require students to choose a major. They could take whatever classes they liked and choose whatever they wanted to put on their degree. Paul chose \"Artificial Intelligence\" as his focus.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"what is paul's life early?\"\n",
        "prompt = text_qa_template.format(context_str=text, query_str=question)\n",
        "response = llm.complete(prompt)\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z2RZRcsHMScC",
        "outputId": "1ea8dfd7-c719-4deb-89c4-5670bf5465dc"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Based on the context information provided, Paul's life early on involved working on writing and programming. He wrote short stories and worked on programming using the IBM 1401 computer in 9th grade. He later got a microcomputer and started programming on it, writing simple games and a word processor. He initially planned to study philosophy in college but switched to AI. He taught himself Lisp and reverse-engineered SHRDLU for his undergraduate thesis. However, during his first year of grad school, he realized that AI, as practiced at the time, was a hoax. He then decided to focus on Lisp and wrote a book about Lisp hacking.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"what is paul's life early?\"\n",
        "existing_answer = \"\"\"\n",
        "ased on the context information provided, Paul's life early on involved working on writing and programming. He wrote short stories and worked on programming using the IBM 1401 computer in 9th grade. He later got a microcomputer and started programming on it, writing simple games and a word processor. He initially planned to study philosophy in college but switched to AI. He taught himself Lisp and reverse-engineered SHRDLU for his undergraduate thesis. However, during his first year of grad school, he realized that AI, as practiced at the time, was a hoax. He then decided to focus on Lisp and wrote a book about Lisp hacking\n",
        "\"\"\"\n",
        "prompt = refine_template.format(context_msg=text, query_str=question, existing_answer=existing_answer)\n",
        "response = llm.complete(prompt)\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7tLO2vqtMm0C",
        "outputId": "b9047c1f-9c9b-48f1-fd9a-171728682067"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Based on the new context, Paul's life early on involved a strong interest in writing and programming. He wrote short stories and began programming on the IBM 1401 computer in 9th grade. He later acquired a microcomputer and started programming on it, creating simple games and a word processor. Initially planning to study philosophy in college, he switched to AI and taught himself Lisp. He reverse-engineered SHRDLU for his undergraduate thesis. However, during his first year of grad school, he realized that AI, as practiced at the time, was a hoax. This led him to focus on Lisp and write a book about Lisp hacking.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.llms import ChatMessage\n",
        "\n",
        "chat_history = [\n",
        "    ChatMessage(role=\"system\", content=\"You are a helpful QA chatbot that can answer questions about life of paul gram.\"),\n",
        "    ChatMessage(role=\"user\", content=\"why paul choose AI\"),\n",
        "]\n",
        "\n",
        "response = llm.chat(chat_history)\n",
        "print(response.message)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ez_hkLb_NQcB",
        "outputId": "5e15587d-d239-4ac9-f411-60f233359494"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "assistant: Paul Graham, the co-founder of Y Combinator, has a strong interest in artificial intelligence (AI) for several reasons. \n",
            "\n",
            "Firstly, AI has the potential to revolutionize various industries and solve complex problems. Paul believes that AI can bring about significant advancements in areas such as healthcare, transportation, and education. By investing in AI, Paul aims to support and promote the development of technologies that can have a positive impact on society.\n",
            "\n",
            "Secondly, Paul recognizes the economic potential of AI. He understands that AI has the power to create new industries and job opportunities. By focusing on AI, Paul aims to foster innovation and entrepreneurship, which can lead to economic growth and prosperity.\n",
            "\n",
            "Lastly, Paul has a genuine fascination with the field of AI. He is intrigued by the possibilities and challenges that AI presents. Paul enjoys exploring and understanding the capabilities of AI systems and believes that it is an exciting area of research and development.\n",
            "\n",
            "Overall, Paul Graham's choice to focus on AI stems from his belief in its transformative potential, economic opportunities, and personal interest in the field.\n"
          ]
        }
      ]
    }
  ]
}